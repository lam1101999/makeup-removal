{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from IPython.display import clear_output\n",
    "from torchvision import transforms\n",
    "from collections import namedtuple, deque\n",
    "from PIL import Image\n",
    "from typing import Any\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models import Discriminator, Generator, weights_init\n",
    "from dataset import GeneratorDataset, ImageBuffer\n",
    "from torch.optim import lr_scheduler\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INIT PARAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = 96\n",
    "SCALE_WIDTH = 96\n",
    "# Dataset path\n",
    "base_logdir = os.path.join(os.getcwd(), \"logs_gan\")\n",
    "OUTPUT_PATH = os.path.join(os.getcwd(), \"output\")\n",
    "\n",
    "USE_BUFFER = True  # Use image buffer to train discriminator\n",
    "REPLAY_PROB = 0.5  # The probability of using previous fake images to train discriminator\n",
    "BUFFER_SIZE = 50  # The maximum size of image buffer\n",
    "BATCH_SIZE = 1\n",
    "EPOCH = 200\n",
    "CURRENT_EPOCH = 1  # Epoch start from\n",
    "SAVE_EVERY_N_EPOCH = 5  # Save checkpoint at every n epoch\n",
    "\n",
    "# Discriminator loss will be multiplied by this weight\n",
    "DISCRIMINATOR_LOSS_WEIGHT = 0.5\n",
    "# The label of fake label will be generated within this range.\n",
    "SOFT_FAKE_LABEL_RANGE = [0.0, 0.3]\n",
    "# The label of real label will be generated within this range.\n",
    "SOFT_REAL_LABEL_RANGE = [0.7, 1.2]\n",
    "LR = 0.0002\n",
    "LR_DECAY_EPOCH = 100\n",
    "LAMBDA = 10  # loss weight for cycle consistency\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter(base_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(images, std=0.5, mean=0.5):\n",
    "    # For plot\n",
    "    images = (images * std) + mean\n",
    "    return images\n",
    "\n",
    "\n",
    "def deprocess(input_tensor):\n",
    "    if len(input_tensor.shape) == 3:\n",
    "        return np.transpose(denormalize(input_tensor.to(device).cpu()), (1, 2, 0))\n",
    "    elif len(input_tensor.shape) == 4:\n",
    "        return np.transpose(denormalize(input_tensor.to(device).cpu()), (0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_XtoY = Generator().to(device)\n",
    "G_YtoX = Generator().to(device)\n",
    "G_XtoY.apply(weights_init)\n",
    "G_YtoX.apply(weights_init)\n",
    "\n",
    "D_X = Discriminator().to(device)\n",
    "D_Y = Discriminator().to(device)\n",
    "D_X.apply(weights_init)\n",
    "D_Y.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess_train_transformations = transforms.Compose([\n",
    "    transforms.CenterCrop(INPUT_SHAPE),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "preprocess_test_transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "list_dir_data_X = [\n",
    "    os.path.join(os.path.dirname(os.getcwd()),\n",
    "                 \"dataset\", \"mtdataset_96\", \"with\"),\n",
    "    os.path.join(os.path.dirname(os.getcwd()), \"dataset\", \"gan_makeup_data_96\", \"with\")]\n",
    "data_X = GeneratorDataset(\n",
    "    list_dir_data_X, transform=preprocess_train_transformations)\n",
    "train_data_X, test_data_X = random_split(data_X, [0.9, 0.1])\n",
    "\n",
    "list_dir_data_Y = [\n",
    "    os.path.join(os.path.dirname(os.getcwd()),\n",
    "                 \"dataset\", \"mtdataset_96\", \"without\"),\n",
    "    os.path.join(os.path.dirname(os.getcwd()), \"dataset\", \"gan_makeup_data_96\", \"without\")]\n",
    "data_Y = GeneratorDataset(\n",
    "    list_dir_data_Y, transform=preprocess_train_transformations)\n",
    "train_data_Y, test_data_Y = random_split(data_Y, [0.9, 0.1])\n",
    "\n",
    "train_dataloader_X = DataLoader(\n",
    "    train_data_X, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader_X = DataLoader(\n",
    "    test_data_X, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_dataloader_Y = DataLoader(\n",
    "    train_data_Y, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader_Y = DataLoader(\n",
    "    test_data_Y, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(train_data_X), len(test_data_X), len(train_data_Y), len(test_data_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train_image_X = iter(train_dataloader_X)\n",
    "iter_train_image_Y = iter(train_dataloader_Y)\n",
    "iter_test_image_X = iter(test_dataloader_X)\n",
    "iter_test_image_Y = iter(test_dataloader_Y)\n",
    "sample_X = next(iter_test_image_X)\n",
    "sample_Y = next(iter_test_image_Y)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].set_title('X')\n",
    "axs[0].imshow(deprocess(sample_X)[0])\n",
    "axs[1].set_title('Y')\n",
    "axs[1].imshow(deprocess(sample_Y)[0])\n",
    "\n",
    "image_buffer = ImageBuffer(buffer_size=BUFFER_SIZE)\n",
    "training_steps = min(len(train_data_X), len(train_data_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_XtoY_optimizer = torch.optim.Adam(\n",
    "    G_XtoY.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "G_YtoX_optimizer = torch.optim.Adam(\n",
    "    G_YtoX.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "D_X_optimizer = torch.optim.Adam(\n",
    "    D_X.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "D_Y_optimizer = torch.optim.Adam(\n",
    "    D_Y.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "def lambda_rule(epoch):\n",
    "    lr = 1.0 - max(0, epoch + 1 - LR_DECAY_EPOCH) / float(LR_DECAY_EPOCH + 1)\n",
    "    return lr\n",
    "\n",
    "\n",
    "D_X_optimizer_scheduler = lr_scheduler.LambdaLR(\n",
    "    D_X_optimizer, lr_lambda=lambda_rule)\n",
    "D_Y_optimizer_scheduler = lr_scheduler.LambdaLR(\n",
    "    D_Y_optimizer, lr_lambda=lambda_rule)\n",
    "G_XtoY_optimizer_scheduler = lr_scheduler.LambdaLR(\n",
    "    G_XtoY_optimizer, lr_lambda=lambda_rule)\n",
    "G_YtoX_optimizer_scheduler = lr_scheduler.LambdaLR(\n",
    "    G_YtoX_optimizer, lr_lambda=lambda_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_image_vector, generated_image_vector):\n",
    "    real_loss = (real_image_vector - torch.FloatTensor(real_image_vector.size()).uniform_(SOFT_REAL_LABEL_RANGE[0],\n",
    "                                                                                          SOFT_REAL_LABEL_RANGE[1]).to(device)).pow(2).mean()\n",
    "    fake_loss = (generated_image_vector - torch.FloatTensor(generated_image_vector.size()).uniform_(SOFT_FAKE_LABEL_RANGE[0],\n",
    "                                                                                                    SOFT_FAKE_LABEL_RANGE[1]).to(device)).pow(2).mean()\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss * 0.5\n",
    "\n",
    "\n",
    "def generator_loss(generated_image):\n",
    "    loss = (generated_image - torch.FloatTensor(generated_image.size()).uniform_(SOFT_REAL_LABEL_RANGE[0],\n",
    "                                                                                 SOFT_REAL_LABEL_RANGE[1]).to(device)).pow(2).mean()\n",
    "    return loss\n",
    "\n",
    "# def discriminator_loss(real_image, generated_image):\n",
    "#     pass\n",
    "\n",
    "\n",
    "# def generator_loss(generated_image):\n",
    "#     pass\n",
    "\n",
    "\n",
    "def cycle_consistency_loss(real_image, cycled_image):\n",
    "    loss = (real_image - cycled_image).pow(2).mean()\n",
    "    return loss * LAMBDA\n",
    "\n",
    "\n",
    "def identity_loss(real_image, generated_image):\n",
    "    loss = (real_image - generated_image).abs().mean()\n",
    "    return loss * 0.5 * LAMBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "real_image_vector = torch.randn((1, 3, 96, 96)).cuda()\n",
    "fake_image = torch.randn((1, 3, 96, 96)).cuda()\n",
    "print(cycle_consistency_loss(real_image_vector, fake_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Check Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getcwd(), \"checkpoints\", 'gan')\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "\n",
    "\n",
    "def save_training_checkpoint(epoch):\n",
    "    state_dict = {\n",
    "        'G_XtoY': G_XtoY.state_dict(),\n",
    "        'G_YtoX': G_YtoX.state_dict(),\n",
    "        'D_X': D_X.state_dict(),\n",
    "        'D_Y': D_Y.state_dict(),\n",
    "        'G_XtoY_optimizer': G_XtoY_optimizer.state_dict(),\n",
    "        'G_YtoX_optimizer': G_YtoX_optimizer.state_dict(),\n",
    "        'D_X_optimizer': D_X_optimizer.state_dict(),\n",
    "        'D_Y_optimizer': D_Y_optimizer.state_dict(),\n",
    "        'D_X_optimizer_scheduler': D_X_optimizer_scheduler.state_dict(),\n",
    "        'D_Y_optimizer_scheduler': D_Y_optimizer_scheduler.state_dict(),\n",
    "        'G_XtoY_optimizer_scheduler': G_XtoY_optimizer_scheduler.state_dict(),\n",
    "        'G_YtoX_optimizer_scheduler': G_YtoX_optimizer_scheduler.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }\n",
    "\n",
    "    save_path = os.path.join(checkpoint_path, 'training-checkpoint')\n",
    "    torch.save(state_dict, save_path)\n",
    "\n",
    "\n",
    "def save_models():\n",
    "    state_dict = {\n",
    "        'G_XtoY': G_XtoY,\n",
    "        'G_YtoX': G_YtoX\n",
    "    }\n",
    "    save_path = os.path.join(checkpoint_path, 'model')\n",
    "    torch.save(state_dict, checkpoint_path)\n",
    "\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if os.path.isfile(os.path.join(checkpoint_path, 'training-checkpoint')):\n",
    "    checkpoint = torch.load(os.path.join(\n",
    "        checkpoint_path, 'training-checkpoint'))\n",
    "    G_XtoY.load_state_dict(checkpoint['G_XtoY'])\n",
    "    G_YtoX.load_state_dict(checkpoint['G_YtoX'])\n",
    "    D_X.load_state_dict(checkpoint['D_X'])\n",
    "    D_Y.load_state_dict(checkpoint['D_Y'])\n",
    "    G_XtoY_optimizer.load_state_dict(checkpoint['G_XtoY_optimizer'])\n",
    "    G_YtoX_optimizer.load_state_dict(checkpoint['G_YtoX_optimizer'])\n",
    "    D_X_optimizer.load_state_dict(checkpoint['D_X_optimizer'])\n",
    "    D_Y_optimizer.load_state_dict(checkpoint['D_Y_optimizer'])\n",
    "    D_X_optimizer_scheduler.load_state_dict(\n",
    "        checkpoint['D_X_optimizer_scheduler'])\n",
    "    D_Y_optimizer_scheduler.load_state_dict(\n",
    "        checkpoint['D_Y_optimizer_scheduler'])\n",
    "    G_XtoY_optimizer_scheduler.load_state_dict(\n",
    "        checkpoint['G_XtoY_optimizer_scheduler'])\n",
    "    G_YtoX_optimizer_scheduler.load_state_dict(\n",
    "        checkpoint['G_YtoX_optimizer_scheduler'])\n",
    "    CURRENT_EPOCH = checkpoint['epoch']\n",
    "    print('Latest checkpoint of epoch {} restored!!'.format(CURRENT_EPOCH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, img_name='img', step=0):\n",
    "    prediction = model(test_input.to(device)).cpu().detach()\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    display_list = [test_input, prediction]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "    # Using the file writer, log the reshaped image.\n",
    "    writer.add_image(os.path.join('train', img_name),\n",
    "                     denormalize(prediction)[0].numpy(), step)\n",
    "    writer.flush()\n",
    "\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(deprocess(display_list[i])[0])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_test_images(model1, model2, test_input, img_name='img', step=0, show_result=False):\n",
    "    '''\n",
    "        Generate images and cycled images, then save them to tensorboard\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        prediction1 = model1(test_input.to(device))\n",
    "        prediction2 = model2(prediction1)\n",
    "\n",
    "    test_input = test_input.cpu()\n",
    "    prediction1 = prediction1.cpu()\n",
    "    prediction2 = prediction2.cpu()\n",
    "    display_list = [test_input, prediction1, prediction2]\n",
    "    title = ['Input Image', 'Predicted Image', 'Cycled Image']\n",
    "\n",
    "    writer.add_image(os.path.join('test', img_name, ' original'),\n",
    "                     denormalize(test_input)[0].numpy(), step)\n",
    "    writer.add_image(os.path.join('test', img_name, ' predicted'),\n",
    "                     denormalize(prediction1)[0].numpy(), step)\n",
    "    writer.add_image(os.path.join('test', img_name, ' cycled'),\n",
    "                     denormalize(prediction2)[0].numpy(), step)\n",
    "    writer.flush()\n",
    "\n",
    "    if show_result:\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            plt.title(title[i])\n",
    "            # getting the pixel values between [0, 1] to plot it.\n",
    "            plt.imshow(deprocess(display_list[i])[0])\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def save_test_images(model1, model2, test_input, folder_name='img', step=0, save=False, show_result=False):\n",
    "    '''\n",
    "        Generate images and cycled images, then save them as jpg\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        prediction1 = model1(test_input.to(device))\n",
    "        prediction2 = model2(prediction1)\n",
    "\n",
    "    test_input = test_input.cpu()\n",
    "    prediction1 = prediction1.cpu()\n",
    "    prediction2 = prediction2.cpu()\n",
    "\n",
    "    display_list = [test_input, prediction1, prediction2]\n",
    "    title = ['original', 'predicted', 'cycled']\n",
    "    figure_title = ['Input Image', 'Predicted Image', 'Cycled Image']\n",
    "\n",
    "    base_folder = os.path.join(OUTPUT_PATH, folder_name)\n",
    "    if not os.path.exists(base_folder):\n",
    "        os.makedirs(base_folder)\n",
    "\n",
    "    if save:\n",
    "        for img, title in zip(display_list, title):\n",
    "            save_folder = os.path.join(base_folder, title)\n",
    "            if not os.path.exists(save_folder):\n",
    "                os.makedirs(save_folder)\n",
    "            img = deprocess(img)[0]\n",
    "            plt.imsave(os.path.join(save_folder, '{}.jpg'.format(step)), img)\n",
    "\n",
    "    if show_result:\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            plt.title(figure_title[i])\n",
    "            # getting the pixel values between [0, 1] to plot it.\n",
    "            plt.imshow(deprocess(display_list[i])[0])\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class TimeCounter():\n",
    "    def __init__(self):\n",
    "        self.init_time = time.time()\n",
    "        self.stored_time = self.init_time\n",
    "\n",
    "    def count(self, task_name=\"task\"):\n",
    "        current_time = time.time()\n",
    "        consumed_time = current_time - self.stored_time\n",
    "        self.stored_time = current_time\n",
    "        print('Task {}, consume {} sec'.format(task_name, consumed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_XtoY.summary()\n",
    "# G_YtoX.summary()\n",
    "# D_X.summary()\n",
    "# D_Y.summary()\n",
    "# # Write graph\n",
    "# writer.add_graph(G_XtoY, sample_X.to(device))\n",
    "# writer.add_graph(G_YtoX, sample_Y.to(device))\n",
    "# writer.add_graph(D_X, sample_X.to(device))\n",
    "# writer.add_graph(D_Y, sample_Y.to(device))\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "current_learning_rate = LR\n",
    "\n",
    "trace = True\n",
    "for epoch in range(CURRENT_EPOCH, EPOCH + 1):\n",
    "    start = time.time()\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    # Reset dataloader\n",
    "    iter_train_image_X = iter(train_dataloader_X)\n",
    "    iter_train_image_Y = iter(train_dataloader_Y)\n",
    "    # Average the losses of an epoch and write them.\n",
    "    G_XtoY_loss_mean = 0\n",
    "    G_YtoX_loss_mean = 0\n",
    "    D_X_loss_mean = 0\n",
    "    D_Y_loss_mean = 0\n",
    "    for step in range(training_steps):\n",
    "\n",
    "        # The probability to use previous images to train discriminators\n",
    "        replay_previous = True if REPLAY_PROB > random.random() else False\n",
    "\n",
    "        real_image_X = next(iter_train_image_X).to(device)\n",
    "        real_image_Y = next(iter_train_image_Y).to(device)\n",
    "\n",
    "        # Generate fake images for discriminators\n",
    "        fake_image_X = G_YtoX(real_image_Y).detach()\n",
    "        fake_image_Y = G_XtoY(real_image_X).detach()\n",
    "\n",
    "        image_buffer.add(real_image_X, fake_image_X,\n",
    "                         real_image_Y, fake_image_Y)\n",
    "\n",
    "        D_X_optimizer.zero_grad()\n",
    "        D_Y_optimizer.zero_grad()\n",
    "        if USE_BUFFER and replay_previous:\n",
    "            # Get previous generated fake images\n",
    "            buffered_images = image_buffer.sample()\n",
    "            # Compute the discriminator loss using buffered images\n",
    "            real_buffer_image_X = buffered_images.real_image_X\n",
    "            fake_buffer_image_X = buffered_images.fake_image_X\n",
    "            real_buffer_image_Y = buffered_images.real_image_Y\n",
    "            fake_buffer_image_Y = buffered_images.fake_image_Y\n",
    "\n",
    "            D_X_real_buffer = D_X(real_buffer_image_X)\n",
    "            D_X_fake_buffer = D_X(fake_buffer_image_X)\n",
    "            D_X_loss = discriminator_loss(D_X_real_buffer, D_X_fake_buffer)\n",
    "\n",
    "            D_Y_real_buffer = D_Y(real_buffer_image_Y)\n",
    "            D_Y_fake_buffer = D_Y(fake_buffer_image_Y)\n",
    "            D_Y_loss = discriminator_loss(D_Y_real_buffer, D_Y_fake_buffer)\n",
    "        else:\n",
    "            # Compute the discriminator loss using the latest fake images\n",
    "            D_X_real = D_X(real_image_X)\n",
    "            D_X_fake = D_X(fake_image_X)\n",
    "            D_Y_real = D_Y(real_image_Y)\n",
    "            D_Y_fake = D_Y(fake_image_Y)\n",
    "            D_X_loss = discriminator_loss(D_X_real, D_X_fake)\n",
    "            D_Y_loss = discriminator_loss(D_Y_real, D_Y_fake)\n",
    "\n",
    "        # ============================\n",
    "        # Update discriminators\n",
    "        # ============================\n",
    "        D_X_loss.backward()\n",
    "        D_Y_loss.backward()\n",
    "        D_X_optimizer.step()\n",
    "        D_Y_optimizer.step()\n",
    "\n",
    "        # ============================\n",
    "        # Compute the generator loss\n",
    "        # ============================\n",
    "        G_XtoY_optimizer.zero_grad()\n",
    "        G_YtoX_optimizer.zero_grad()\n",
    "\n",
    "        fake_image_Y = G_XtoY(real_image_X)\n",
    "        fake_image_X = G_YtoX(real_image_Y)\n",
    "        dis_fake_image_Y = D_Y(fake_image_Y)\n",
    "        dis_fake_image_X = D_X(fake_image_X)\n",
    "\n",
    "        G_XtoY_loss = generator_loss(dis_fake_image_Y)\n",
    "        G_YtoX_loss = generator_loss(dis_fake_image_X)\n",
    "        # ============================\n",
    "        # Compute the cycle consistency loss\n",
    "        # ============================\n",
    "        cycled_XtoYtoX = G_YtoX(fake_image_Y)\n",
    "        cycled_YtoXtoY = G_XtoY(fake_image_X)\n",
    "        cycled_XtoY_loss = cycle_consistency_loss(real_image_X, cycled_XtoYtoX)\n",
    "        cycled_YtoX_loss = cycle_consistency_loss(real_image_Y, cycled_YtoXtoY)\n",
    "        total_cycle_loss = cycled_XtoY_loss + cycled_YtoX_loss\n",
    "\n",
    "        # ============================\n",
    "        # Compute the identity loss\n",
    "        # ============================\n",
    "        same_image_Y = G_XtoY(real_image_Y)\n",
    "        same_image_X = G_YtoX(real_image_X)\n",
    "        identity_loss_for_YtoX = identity_loss(real_image_X, same_image_X)\n",
    "        identity_loss_for_XtoY = identity_loss(real_image_Y, same_image_Y)\n",
    "\n",
    "        # ============================\n",
    "        # Combine all generator losses\n",
    "        # ============================\n",
    "        total_G_XtoY_loss = G_XtoY_loss + identity_loss_for_XtoY\n",
    "        total_G_YtoX_loss = G_YtoX_loss + identity_loss_for_YtoX\n",
    "        total_G_losses = total_G_XtoY_loss + total_G_YtoX_loss + total_cycle_loss\n",
    "        # ============================\n",
    "        # Update generators\n",
    "        # ============================\n",
    "        total_G_losses.backward()\n",
    "        G_XtoY_optimizer.step()\n",
    "        G_YtoX_optimizer.step()\n",
    "\n",
    "        # Add losses\n",
    "        G_XtoY_loss_mean = G_XtoY_loss_mean + \\\n",
    "            total_G_XtoY_loss.item() + total_cycle_loss.item()\n",
    "        G_YtoX_loss_mean = G_YtoX_loss_mean + \\\n",
    "            total_G_YtoX_loss.item() + total_cycle_loss.item()\n",
    "        D_X_loss_mean += D_X_loss.item()\n",
    "        D_Y_loss_mean += D_Y_loss.item()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print('.', end='')\n",
    "\n",
    "    # ============================\n",
    "    # Write scalars at the end of an epoch\n",
    "    # ============================\n",
    "    writer.add_scalar('Loss/total_G_XtoY_loss',\n",
    "                      G_XtoY_loss_mean / training_steps, epoch)\n",
    "    writer.add_scalar('Loss/total_G_YtoX_loss',\n",
    "                      G_YtoX_loss_mean / training_steps, epoch)\n",
    "    writer.add_scalar('Loss/D_X_loss', D_X_loss_mean / training_steps, epoch)\n",
    "    writer.add_scalar('Loss/D_Y_loss', D_Y_loss_mean / training_steps, epoch)\n",
    "    writer.flush()\n",
    "    # ============================\n",
    "    # Update schedulers\n",
    "    # ============================\n",
    "    D_X_optimizer_scheduler.step()\n",
    "    D_Y_optimizer_scheduler.step()\n",
    "    G_XtoY_optimizer_scheduler.step()\n",
    "    G_YtoX_optimizer_scheduler.step()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    # Using a consistent image (sample_X) so that the progress of the model\n",
    "    # is clearly visible.\n",
    "    generate_images(G_XtoY, sample_X, img_name='sample_X', step=epoch)\n",
    "    generate_images(G_YtoX, sample_Y, img_name='sample_Y', step=epoch)\n",
    "\n",
    "    if epoch % SAVE_EVERY_N_EPOCH == 0:\n",
    "        save_training_checkpoint(epoch)\n",
    "        print('Saving checkpoint for epoch {} at {}'.format(epoch,\n",
    "                                                            checkpoint_path))\n",
    "\n",
    "    print('Time taken for epoch {} is {} sec\\n'.format(epoch,\n",
    "                                                       time.time()-start))\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
